{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361a920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"HF_DATASETS_CACHE\"] = \"/domino/datasets/local/SAS-R/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] =  \"/mnt/data/CodeGen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5919fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/data/CodeGen'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils import TRANSFORMERS_CACHE\n",
    "TRANSFORMERS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ef9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36ed6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.07s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'codellama/CodeLlama-7b-Instruct-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=TRANSFORMERS_CACHE)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=TRANSFORMERS_CACHE,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\",\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8074c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5f18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code=\"\"\"\n",
    "/* Load the input dataset */\n",
    "proc import out=cars_data\n",
    "    datafile=\"/mnt/data/mtcars.csv\"\n",
    "    dbms=csv\n",
    "    replace;\n",
    "    getnames=YES;\n",
    "run;\n",
    "\n",
    "/* Sort the input dataset by displacement */\n",
    "proc sort data=cars_data;\n",
    "      by disp;\n",
    "run;\n",
    "\n",
    "/* Perform the linear regression */\n",
    "proc reg data=cars_data;\n",
    "      model mpg = disp / noprint;\n",
    "      output out=output_data predicted=mpg_predicted;\n",
    "run;\n",
    "\n",
    "/* Print the regression results */\n",
    "/* In this case a scatter plot with regression line best fit */\n",
    "proc sgplot data=output_data;\n",
    "      scatter x=disp y=mpg / markerattrs=(symbol=circlefilled) name='scatter';\n",
    "      series x=disp y=mpg_predicted / lineattrs=(color=blue) name='regression';\n",
    "      keylegend 'scatter' 'regression';\n",
    "run;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58e49dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert= \"You are a code assistant.Please write R code using statistics and plotting libraries that will reproduce the results of the SAS code that follows. Output the result in markdown \\n\"\n",
    "thisPrompt=\"{}\\n{} ```r \".format(convert, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba76cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sequences = pipe(\n",
    "    thisPrompt,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45e0e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "library(dplyr)\n",
      "library(MASS)\n",
      "library(sasstats)\n",
      "\n",
      "c <-read.csv(\"path_of_your_data\")\n",
      "\n",
      "cars_sorted <-c %>% \n",
      "  select(disp, mpg) %>% \n",
      "  arrange(desc(disp))\n",
      "\n",
      "call <- lm(mpg ~ disp, data=cars_sorted)\n",
      "summary(call)\n",
      "prediction5<- predict(call,newdata=cars_sorted)\n",
      "output_data<-data.frame(\n",
      "  'disp'= c(cars_sorted$disp),\n",
      " 'mpg'=c(cars_sorted$mpg),\n",
      "  \n",
      " 'mpg_predicted' = as.numeric(prediction5),\n",
      ")\n",
      "library(ggplot2)\n",
      "b<-ggplot(output_data,aes(x=disp,y=mpg))+\n",
      "    geom_point() +\n",
      "    ylab(\"mpg\") +\n",
      "    xlab(\"disp\") +\n",
      "    geom_line(aes(y = mpg_predicted,x = disp),color=\"blue\") +\n",
      "    geom_point(color = \"red\")+\n",
      "    geom_line(aes(y = prediction5,x = disp),color = \"green\")        \n",
      "b\n",
      " ```\n",
      "\n",
      "**Answer**\n",
      "\n",
      "The SAS code is provided above.The plot generated using R  code and output is below:\n",
      "\n",
      "![alt](https://jasonwinn.gitbooks.io/hands_on_statistics_and_data_visualization/content/4/sas/sasviz_graph.png)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    output_text = seq['generated_text']\n",
    "    output_text = output_text.replace(thisPrompt,\"\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc5d0b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "convert= \"You are a helpful polite code assistant.Please write R code using statistics for example stats and plotting libraries for example ggplot for the task that follows. Output the result in markdown \\n Task : \"\n",
    "task = \"Give me code to load data from a csv and perform linear regression once the data has been loaded\"\n",
    "thisPrompt=\"{}\\n{} ```r \".format(convert, task)\n",
    "\n",
    "sequences = pipe(\n",
    "    thisPrompt,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d155f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#load your data\n",
      "mydata <- read.csv(\"file.csv\") \n",
      "str(mydata)\n",
      "\n",
      "\n",
      "#plot and view your data\n",
      "ggplot(mydata,aes(x=x,y=y))+geom_point()\n",
      "\n",
      "\n",
      "#Perform a linear regression of your chosen X and Y\n",
      "summary(lm(mydata$y~mydata$x)) \n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    output_text = seq['generated_text']\n",
    "    output_text = output_text.replace(thisPrompt,\"\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864585e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
